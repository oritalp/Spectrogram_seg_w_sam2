{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e7a0db5-7f04-4845-8b11-684fe6e9f7f2",
   "metadata": {
    "id": "6e7a0db5-7f04-4845-8b11-684fe6e9f7f2"
   },
   "source": [
    "# Spectrogram segmentation with SAM 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ba7875-35e5-478b-b8ba-4b48e121dec7",
   "metadata": {
    "id": "73ba7875-35e5-478b-b8ba-4b48e121dec7"
   },
   "source": [
    "In the following, we will use SAM 2 (a videos and images segmentation model) to segment the spectorgram of a noisy siganl in order to denoise it. There are two options to use SAM 2 for that aim:\n",
    "- Create a single spectorgram of the whole audio signal and segment it.\n",
    "- Create multiple spectorgrams of the audio signal from non overlapping sections and segment them.\n",
    "The former corresponds to stting _single_frame_ to True and the latter to False.\n",
    "\n",
    "At the moment, the model requires prompting the spectrogram with positive and negative clicks. \n",
    "\n",
    "After performing the spectorgram masking, we use the well known diff-wave model to reconstruct the audio signal from the masked spectorgram."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a887b90f-6576-4ef8-964e-76d3a156ccb6",
   "metadata": {
    "id": "a887b90f-6576-4ef8-964e-76d3a156ccb6"
   },
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/facebookresearch/sam2/blob/main/notebooks/video_predictor_example.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26616201-06df-435b-98fd-ad17c373bb4a",
   "metadata": {
    "id": "26616201-06df-435b-98fd-ad17c373bb4a"
   },
   "source": [
    "## 1. Environment Set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8491a127-4c01-48f5-9dc5-f148a9417fdf",
   "metadata": {
    "id": "8491a127-4c01-48f5-9dc5-f148a9417fdf"
   },
   "source": [
    "If running locally using jupyter, first install `sam2` in your environment using the [installation instructions](https://github.com/facebookresearch/sam2#installation) in the repository.\n",
    "\n",
    "If running from Google Colab, set `using_colab=True` below and run the cell. In Colab, be sure to select 'GPU' under 'Edit'->'Notebook Settings'->'Hardware accelerator'. Note that it's recommended to use **A100 or L4 GPUs when running in Colab** (T4 GPUs might also work, but could be slow and might run out of memory in some cases)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f74c53be-aab1-46b9-8c0b-068b52ef5948",
   "metadata": {
    "id": "f74c53be-aab1-46b9-8c0b-068b52ef5948"
   },
   "outputs": [],
   "source": [
    "# Parameters setting:\n",
    "single_frame = True\n",
    "use_pre_loaded_clicks = True\n",
    "sigma = 0.02\n",
    "add_noise = True # When single_frame is True, the segmentation is done on the noisy audio\n",
    "                 # When single_frame is False, the segmentation is done on the first clen frame and then diffused\n",
    "                 # to the next noisy segmentations.\n",
    "deafult_saving_dpi = 600\n",
    "deafult_printing_dpi = 600\n",
    "audio_path = r'audio_example.wav'  # Replace with your audio file path\n",
    "sam2_checkpoint = \"checkpoints/sam2.1_hiera_large.pt\"\n",
    "model_cfg = \"configs/sam2.1/sam2.1_hiera_l.yaml\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d824a4b2-71f3-4da3-bfc7-3249625e6730",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d824a4b2-71f3-4da3-bfc7-3249625e6730",
    "outputId": "666ad841-a007-4410-e88f-6c6d92039341"
   },
   "outputs": [],
   "source": [
    "using_colab = False\n",
    "\n",
    "if using_colab:\n",
    "    import torch\n",
    "    import torchvision\n",
    "    print(\"PyTorch version:\", torch.__version__)\n",
    "    print(\"Torchvision version:\", torchvision.__version__)\n",
    "    print(\"CUDA is available:\", torch.cuda.is_available())\n",
    "    import sys\n",
    "    !{sys.executable} -m pip install opencv-python matplotlib\n",
    "    !{sys.executable} -m pip install 'git+https://github.com/facebookresearch/sam2.git'\n",
    "\n",
    "    !mkdir -p videos\n",
    "    !wget -P videos https://dl.fbaipublicfiles.com/segment_anything_2/assets/bedroom.zip\n",
    "    !unzip -d videos videos/bedroom.zip\n",
    "\n",
    "    !mkdir -p ../checkpoints/\n",
    "    !wget -P ../checkpoints/ https://dl.fbaipublicfiles.com/segment_anything_2/092824/sam2.1_hiera_large.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e6aa9d-487f-4207-b657-8cff0902343e",
   "metadata": {
    "id": "22e6aa9d-487f-4207-b657-8cff0902343e"
   },
   "source": [
    "## Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5318a85-5bf7-4880-b2b3-15e4db24d796",
   "metadata": {
    "id": "e5318a85-5bf7-4880-b2b3-15e4db24d796"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import shutil\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import torchaudio as T\n",
    "import torchaudio.transforms as TT\n",
    "from diffwave.inference import predict as diffwave_predict\n",
    "from diffwave.params import params\n",
    "import librosa.display\n",
    "from sam2.build_sam import build_sam2_video_predictor\n",
    "from pathlib import Path\n",
    "from clicker import collect_clicks\n",
    "import pickle\n",
    "import utils\n",
    "\n",
    "if sys.platform == \"darwin\": # This means if we are using macOS\n",
    "    os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "\n",
    "\n",
    "# setting global parameters for printing and saving the images. If we want to change it we need also to change it at SAM2 so leave it as is for the moment\n",
    "plt.rcParams['figure.figsize'] = [8, 8]\n",
    "plt.rcParams['figure.dpi'] = deafult_printing_dpi\n",
    "\n",
    "single_frame_str = \"single_frame\" if single_frame else \"diffused\"\n",
    "noise_str = \"with_noise\" if add_noise else \"only_clean\"\n",
    "output_dir = Path(\"results\")/ audio_path.split('.')[0] / f\"sigma_{sigma}\" / single_frame_str / noise_str / \"spectorgrams\"\n",
    "clean_output_dir = output_dir / \"clean\"\n",
    "# create a directory with pathlib mkdir of f\"output_audios_for{audio_path (without suffix)}\"\n",
    "output_audio_dir = output_dir.parent / \"output_audios\"\n",
    "\n",
    "prompts_dir = Path(\"prompts\") / audio_path.split('.')[0] / f\"sigma_{sigma}\" / single_frame_str / noise_str / \"clicks.pkl\"\n",
    "\n",
    "#if output_dir or output_audio_dir do exist, delete them and create them again\n",
    "if output_dir.exists():\n",
    "    shutil.rmtree(output_dir)\n",
    "output_dir.mkdir(exist_ok=True, parents = True)\n",
    "\n",
    "if output_audio_dir.exists():\n",
    "    shutil.rmtree(output_audio_dir)\n",
    "output_audio_dir.mkdir(exist_ok=True, parents = True)\n",
    "\n",
    "\n",
    "\n",
    "noisy_audio_path = output_audio_dir / \"noisy_audio.wav\"\n",
    "noisy_output_dir = Path(output_dir) / \"noisy\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ba49d8-8c22-4eba-a2ab-46eee839287f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "08ba49d8-8c22-4eba-a2ab-46eee839287f",
    "outputId": "1598810b-2fd3-444b-ce43-6d7617e3bf25"
   },
   "outputs": [],
   "source": [
    "# select the device for computation\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "# elif torch.backends.mps.is_available():\n",
    "#     device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"using device: {device}\")\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    # use bfloat16 for the entire notebook\n",
    "    torch.autocast(\"cuda\", dtype=torch.bfloat16).__enter__()\n",
    "    # turn on tfloat32 for Ampere GPUs (https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices)\n",
    "    if torch.cuda.get_device_properties(0).major >= 8:\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "elif device.type == \"mps\":\n",
    "    print(\n",
    "        \"\\nSupport for MPS devices is preliminary. SAM 2 is trained with CUDA and might \"\n",
    "        \"give numerically different outputs and sometimes degraded performance on MPS. \"\n",
    "        \"See e.g. https://github.com/pytorch/pytorch/issues/84936 for a discussion.\"\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "# create a noisy version if needed and compute SNR\n",
    "if add_noise is True:\n",
    "    waveform, sample_rate = T.load(audio_path)\n",
    "    signal_power = torch.mean(waveform ** 2)\n",
    "    # Generate Gaussian noise\n",
    "    noise = torch.randn_like(waveform) * sigma \n",
    "    # Add noise\n",
    "    noisy_waveform = waveform + noise\n",
    "    # Normalize to prevent clipping\n",
    "    noisy_waveform = torch.clamp(noisy_waveform, -1.0, 1.0)\n",
    "    # Save the noisy audio\n",
    "    T.save(noisy_audio_path, noisy_waveform, sample_rate)\n",
    "    # Compute SNR\n",
    "    noise_power = torch.mean(noise ** 2)\n",
    "    snr = 10 * torch.log10(signal_power / noise_power)\n",
    "    print(f\"SNR: {snr.item():.2f} dB\")\n",
    "    print(f\"signal power: {signal_power.item():.4f}, noise power: {noise_power.item():.4f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28adaa3f",
   "metadata": {
    "id": "28adaa3f"
   },
   "source": [
    "## 2. Creating a spectrogram from an audio file\n",
    "### There is no need to run this section if the files are already saved on the computer.\n",
    "We will define a function that converts an audio file to spectrogram files and spectrogram images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e077bc",
   "metadata": {
    "id": "08e077bc"
   },
   "source": [
    "Upload an audio file and create a spectrogram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8037794",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d8037794",
    "outputId": "e29e8077-6792-4409-8ad7-3bccf7a669fd"
   },
   "outputs": [],
   "source": [
    "sr, overlap, video_dir = utils.input2mel(audio_path, clean_output_dir, single_frame)\n",
    "\n",
    "print(video_dir)\n",
    "# if add_noise is true, creates mixed dirs where the first frame is clean and the rest are noisy. then, assign video_dir to mixed_video_dir\n",
    "if add_noise is True:\n",
    "    sr, overlap, noisy_video_dir = utils.input2mel(noisy_audio_path, noisy_output_dir, single_frame)\n",
    "    \n",
    "    # Create Path objects for all directories\n",
    "    mixed_images_dir = output_dir / \"mixed\" / \"images\"\n",
    "    mixed_np_dir = output_dir / \"mixed\" / \"np_arrays\"\n",
    "    clean_images_dir = clean_output_dir / \"images\"\n",
    "    clean_np_dir = clean_output_dir / \"np_arrays\"\n",
    "    noisy_images_dir = noisy_output_dir / \"images\"\n",
    "    noisy_np_dir = noisy_output_dir / \"np_arrays\"\n",
    "\n",
    "    \n",
    "    # Create the directories if they don't exist\n",
    "    mixed_images_dir.mkdir(exist_ok=True, parents = True)\n",
    "    mixed_np_dir.mkdir(exist_ok=True, parents = True)\n",
    "    \n",
    "    # Copy files for images using PIL\n",
    "    # Copy 0000.jpg from clean directory\n",
    "    img = Image.open(clean_images_dir / \"0000.jpg\")\n",
    "    img.save(mixed_images_dir / \"0000.jpg\", \"JPEG\", quality=100)\n",
    "    \n",
    "    # Copy all non-0000 files from noisy directory\n",
    "    for image_file in noisy_images_dir.glob(\"*.jpg\"):\n",
    "        if image_file.name != \"0000.jpg\":\n",
    "            img = Image.open(image_file)\n",
    "            img.save(mixed_images_dir / image_file.name, \"JPEG\", quality=100)\n",
    "    \n",
    "    # Copy files for numpy arrays\n",
    "    # Copy 0000.npy from clean directory\n",
    "    shutil.copy2(\n",
    "        clean_np_dir / \"0000.npy\",\n",
    "        mixed_np_dir / \"0000.npy\"\n",
    "    )\n",
    "    \n",
    "    # Copy all non-0000 files from noisy directory\n",
    "    for np_file in noisy_np_dir.glob(\"*.npy\"):\n",
    "        if np_file.name != \"0000.npy\":\n",
    "            shutil.copy2(\n",
    "                np_file,\n",
    "                mixed_np_dir / np_file.name\n",
    "            )\n",
    "\n",
    "    if not single_frame:\n",
    "        video_dir = str(mixed_images_dir)\n",
    "    else:\n",
    "        video_dir = str(noisy_images_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8e0779-751f-4224-9b04-ed0f0b406500",
   "metadata": {
    "id": "ae8e0779-751f-4224-9b04-ed0f0b406500"
   },
   "source": [
    "## 3. Segmentation using SAM2\n",
    "### Loading the SAM 2 video predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f5f3245e-b4d6-418b-a42a-a67e0b3b5aec",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 339
    },
    "id": "f5f3245e-b4d6-418b-a42a-a67e0b3b5aec",
    "outputId": "52726ce5-cdcd-45f3-88bb-23097038dcdb"
   },
   "outputs": [],
   "source": [
    "predictor = build_sam2_video_predictor(model_cfg, sam2_checkpoint, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22aa751-b7cd-451e-9ded-fb98bf4bdfad",
   "metadata": {
    "id": "f22aa751-b7cd-451e-9ded-fb98bf4bdfad"
   },
   "source": [
    "#### Upload the spectrogram images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4c6af6-e18d-4939-beaf-2bc00f94a724",
   "metadata": {
    "id": "1c4c6af6-e18d-4939-beaf-2bc00f94a724"
   },
   "source": [
    "We will upload one picture to make sure everything is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94c87ca-fd1a-4011-9609-e8be1cbe3230",
   "metadata": {
    "id": "b94c87ca-fd1a-4011-9609-e8be1cbe3230",
    "outputId": "e4ab13d1-f297-4d82-8513-3c1f220c332d"
   },
   "outputs": [],
   "source": [
    "# scan all the JPEG frame names in the frames directory\n",
    "frame_names = [\n",
    "    p for p in os.listdir(video_dir)\n",
    "    if os.path.splitext(p)[-1] in [\".jpg\", \".jpeg\", \".JPG\", \".JPEG\"]\n",
    "]\n",
    "frame_names.sort(key=lambda p: int(os.path.splitext(p)[0]))\n",
    "\n",
    "# take a look the first video frame\n",
    "frame_idx = 0\n",
    "plt.title(f\"frame {frame_idx}\")\n",
    "plt.imshow(Image.open(os.path.join(video_dir, frame_names[frame_idx])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff46b10-c17a-4a26-8004-8c6d80806b0a",
   "metadata": {
    "id": "dff46b10-c17a-4a26-8004-8c6d80806b0a"
   },
   "source": [
    "#### Initialize the inference state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f594ac71-a6b9-461d-af27-500fa1d1a420",
   "metadata": {
    "id": "f594ac71-a6b9-461d-af27-500fa1d1a420"
   },
   "source": [
    "SAM 2 requires stateful inference for interactive video segmentation, so we need to initialize an **inference state** on this video.\n",
    "\n",
    "During initialization, it loads all the JPEG frames in `video_path` and stores their pixels in `inference_state` (as shown in the progress bar below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8967aed3-eb82-4866-b8df-0f4743255c2c",
   "metadata": {
    "id": "8967aed3-eb82-4866-b8df-0f4743255c2c",
    "outputId": "a59a0e3c-5cc4-44d9-c9b0-f91ac6c5e1c1"
   },
   "outputs": [],
   "source": [
    "inference_state = predictor.init_state(video_path=video_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2d3127-67b2-45d2-9f32-8fe3e10dc5eb",
   "metadata": {
    "id": "aa2d3127-67b2-45d2-9f32-8fe3e10dc5eb"
   },
   "source": [
    "Note: if you have run any previous tracking using this `inference_state`, please reset it first via `reset_state`.\n",
    "\n",
    "(The cell below is just for illustration; it's not needed to call `reset_state` here as this `inference_state` is just freshly initialized above.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d2646a1d-3401-438c-a653-55e0e56b7d9d",
   "metadata": {
    "id": "d2646a1d-3401-438c-a653-55e0e56b7d9d"
   },
   "outputs": [],
   "source": [
    "predictor.reset_state(inference_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da018be8-a4ae-4943-b1ff-702c2b89cb68",
   "metadata": {
    "id": "da018be8-a4ae-4943-b1ff-702c2b89cb68"
   },
   "source": [
    "### Segment multiple objects simultaneously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c542cfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = {}  # hold all the clicks we add for visualization\n",
    "ann_frame_idx = 0  # the frame index we interact with\\\n",
    "\n",
    "if use_pre_loaded_clicks and prompts_dir.exists():\n",
    "    # Load the pre-loaded clicks\n",
    "    points_list, labels = utils.load_lists(prompts_dir)\n",
    "\n",
    "else:\n",
    "    img_path = os.path.join(video_dir, frame_names[ann_frame_idx])\n",
    "    points_list, labels = collect_clicks(img_path)\n",
    "    prompts_dir.parent.mkdir(exist_ok=True, parents=True)\n",
    "    utils.save_lists(points_list, labels, prompts_dir)\n",
    "\n",
    "assert len(points_list) == len(labels), \"Number of points and labels should be the same\"\n",
    "\n",
    "num_of_promt = len(points_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19166c4d",
   "metadata": {
    "id": "19166c4d"
   },
   "source": [
    "#### Step 1: Show the mask created on the first frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ecf61d-662b-4f98-ae62-46557b219842",
   "metadata": {
    "id": "95ecf61d-662b-4f98-ae62-46557b219842",
    "outputId": "481be066-a984-4d09-8ed9-3137e52abd3a"
   },
   "outputs": [],
   "source": [
    "# # Example of how the points and labels should be set up manually\n",
    "\n",
    "# # sending all clicks (and their labels) to `add_new_points_or_box`\n",
    "# #points_list = [np.array([[600, 170], [275, 800]], dtype=np.float32)] # Exempale\n",
    "# points_list = [np.array([[800, 2100]], dtype=np.float32), np.array([[1500, 2120]], dtype=np.float32)] #TODO: add clicks\n",
    "# num_of_promt = len(points_list)\n",
    "\n",
    "# # for labels, `1` means positive click and `0` means negative click\n",
    "# #labels = [np.array([1, 0], np.int32),np.array([1], np.int32)] # Exempale\n",
    "# labels = [np.array([1], np.int32),np.array([1], np.int32)] #TODO: add labels\n",
    "\n",
    "for i in range(num_of_promt):\n",
    "    prompts[i] = points_list[i], labels[i]\n",
    "    _, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(\n",
    "        inference_state=inference_state,\n",
    "        frame_idx=0,\n",
    "        obj_id=i,\n",
    "        points=points_list[i],\n",
    "        labels=labels[i],\n",
    ")\n",
    "\n",
    "\n",
    "img = Image.open(os.path.join(video_dir, frame_names[ann_frame_idx]))\n",
    "w, h = img.size\n",
    "aspect_ratio = w / h\n",
    "\n",
    "# Create figure with the correct aspect ratio\n",
    "fig = plt.figure()\n",
    "ax = plt.Axes(fig, [0., 0., 1., 1.])\n",
    "fig.add_axes(ax)\n",
    "\n",
    "# Add title (optionally adjust its position if needed)\n",
    "ax.set_title(f\"frame {ann_frame_idx}\", pad=20)\n",
    "\n",
    "# Show image and rest of visualization\n",
    "ax.imshow(img)\n",
    "for i in range(num_of_promt):\n",
    "    utils.show_points(points_list[i], labels[i], ax)\n",
    "    for i, out_obj_id in enumerate(out_obj_ids):\n",
    "        utils.show_points(*prompts[i], ax)\n",
    "        utils.show_mask((out_mask_logits[i] > 0.0).cpu().numpy(), ax, obj_id=out_obj_id)\n",
    "\n",
    "ax.axis('off')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448733b8-ea8b-4078-995f-b676c3b558ba",
   "metadata": {
    "id": "448733b8-ea8b-4078-995f-b676c3b558ba"
   },
   "source": [
    "#### Step 2: Propagate the prompts to get masklets across the video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bd73de-d669-41c8-b6ba-943883f0caa2",
   "metadata": {
    "id": "60bd73de-d669-41c8-b6ba-943883f0caa2"
   },
   "source": [
    "Now, we propagate the prompts for both objects to get their masklets throughout the video.\n",
    "\n",
    "Note: when there are multiple objects, the `propagate_in_video` API will return a list of masks for each object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17737191-d62b-4611-b2c6-6d0418a9ab74",
   "metadata": {
    "id": "17737191-d62b-4611-b2c6-6d0418a9ab74",
    "outputId": "39da9e09-da2f-4de9-810b-882407541f54"
   },
   "outputs": [],
   "source": [
    "\n",
    "# run propagation throughout the video and collect the results in a dict\n",
    "video_segments = {}  # video_segments contains the per-frame segmentation results\n",
    "for out_frame_idx, out_obj_ids, out_mask_logits in predictor.propagate_in_video(inference_state):\n",
    "    video_segments[out_frame_idx] = {\n",
    "        out_obj_id: (out_mask_logits[i] > 0.0).cpu().numpy()\n",
    "        for i, out_obj_id in enumerate(out_obj_ids)\n",
    "    }\n",
    "\n",
    "# render the segmentation results every few frames\n",
    "vis_frame_stride = 1 # Should be reduced if the number of frames is large\n",
    "\n",
    "plt.ioff()  # Turn off interactive mode for printing in the for loop\n",
    "\n",
    "for out_frame_idx in range(0, len(frame_names), vis_frame_stride):\n",
    "    # Create a new figure for each frame\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    plt.title(f\"frame {out_frame_idx}\")\n",
    "    plt.imshow(Image.open(os.path.join(video_dir, frame_names[out_frame_idx])))\n",
    "    \n",
    "    for out_obj_id, out_mask in video_segments[out_frame_idx].items():\n",
    "        utils.show_mask(out_mask, plt.gca(), obj_id=out_obj_id)\n",
    "    \n",
    "    plt.show()  # Display the current figure\n",
    "    plt.close(fig)  # Close the figure to free memory\n",
    "\n",
    "plt.ion()  # Turn interactive mode back on"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9454d0",
   "metadata": {
    "id": "ec9454d0"
   },
   "source": [
    "## 4. Applying the masks to the original spectrograms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973fa1d2",
   "metadata": {
    "id": "973fa1d2"
   },
   "source": [
    "Now we will fit the mask that sam made to the original spectrogram.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e49342a",
   "metadata": {
    "id": "6e49342a"
   },
   "source": [
    "### We will upload the original spectrograms and apply the mask on them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcb5bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "### apply the masks to each frame\n",
    "\n",
    "\n",
    "# Directory containing spectrogram numpy arrays\n",
    "if single_frame and add_noise:\n",
    "    output_dir_spec = noisy_output_dir / \"np_arrays\"\n",
    "elif add_noise is True:\n",
    "    output_dir_spec = Path(output_dir) / \"mixed\" / \"np_arrays\"\n",
    "else:\n",
    "    output_dir_spec = Path(output_dir) / \"clean\" / \"np_arrays\"    \n",
    "\n",
    "# Get and sort spectrogram file names\n",
    "spectrogram_files = [\n",
    "    p for p in os.listdir(output_dir_spec) if p.endswith(\".npy\")\n",
    "]\n",
    "spectrogram_files.sort(key=lambda p: int(os.path.splitext(p)[0]))\n",
    "\n",
    "# Initialize the full masked spectrogram\n",
    "maskt_Sxx_full = []\n",
    "\n",
    "for i, file_name in enumerate(spectrogram_files):\n",
    "    file_path = os.path.join(output_dir_spec, file_name)\n",
    "\n",
    "    # Load spectrogram data\n",
    "    Sxx = np.load(file_path)\n",
    "    freq_bins, time_bins = Sxx.shape\n",
    "\n",
    "    # Matrix-wise processing for all masks\n",
    "    inx_list = []\n",
    "    for j in range(num_of_promt):\n",
    "        mask = video_segments[i][j][0]\n",
    "        mask_height, mask_width = mask.shape\n",
    "\n",
    "        # Extract indices where the mask is True\n",
    "        mask_inx = np.where(mask)  # Returns arrays of y and x indices\n",
    "        mask_x, mask_y = mask_inx[1], mask_inx[0]  # Extract x and y coordinates\n",
    "\n",
    "        # Use matrix-wise function to map pixel coordinates to spectrogram indices\n",
    "        spec_time_bins, spec_freq_bins = utils.matrix_to_spectrogram(\n",
    "            mask_x, mask_y, mask_height, mask_width, time_bins, freq_bins\n",
    "        )\n",
    "\n",
    "        # Add unique indices to the global list\n",
    "        unique_indices = np.unique(np.stack((spec_time_bins, spec_freq_bins), axis=1), axis=0)\n",
    "        inx_list.append(unique_indices)\n",
    "\n",
    "    # Combine all unique indices for this spectrogram\n",
    "    inx_array = np.vstack(inx_list)\n",
    "\n",
    "    # Create the spectrogram mask\n",
    "    alpha = 1/2\n",
    "    mask_to_spec = alpha*np.ones(Sxx.shape)\n",
    "    mask_to_spec[inx_array[:, 1], inx_array[:, 0]] = 1\n",
    "\n",
    "    # Apply mask to the spectrogram\n",
    "    maskt_Sxx = np.multiply(Sxx,mask_to_spec)\n",
    "\n",
    "    # Append to full spectrogram list\n",
    "    maskt_Sxx_full.append(maskt_Sxx)\n",
    "\n",
    "    # Plot the masked spectrogram\n",
    "    plt.figure(figsize=(4, 4), dpi=300)\n",
    "    librosa.display.specshow(\n",
    "        maskt_Sxx,\n",
    "        sr=sr,\n",
    "        hop_length=overlap,\n",
    "        x_axis=\"time\",\n",
    "        y_axis=\"mel\",\n",
    "        cmap=\"magma\",\n",
    "    )\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "# Concatenate all spectrograms along the time axis\n",
    "maskt_Sxx_full = np.concatenate(maskt_Sxx_full, axis=1)\n",
    "print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe68a9ad",
   "metadata": {
    "id": "fe68a9ad"
   },
   "source": [
    "Let's see the final maskt spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaab43eb",
   "metadata": {
    "id": "aaab43eb",
    "outputId": "61c54664-67d1-4650-8e2c-fd7f4e199b0d"
   },
   "outputs": [],
   "source": [
    "# Plot the spectrogram using librosa\n",
    "fig = plt.figure()  # 1024x1024 pixels at 300 DPI\n",
    "ax = plt.Axes(fig, [0., 0., 1., 1.])  # Remove margins for full screen\n",
    "fig.add_axes(ax)\n",
    "ax.axis(\"off\")\n",
    "\n",
    "librosa.display.specshow(\n",
    "    maskt_Sxx_full,\n",
    "    sr=sr,\n",
    "    hop_length=overlap,\n",
    "    x_axis=\"time\",\n",
    "    y_axis=\"mel\",\n",
    "    cmap=\"magma\",  # Can try also 'jet'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1087dbd",
   "metadata": {
    "id": "d1087dbd"
   },
   "source": [
    "# 5. Reconstruction the audio from the maskt spectrograms using DiffWave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HgjXWy54NL3G",
   "metadata": {
    "id": "HgjXWy54NL3G"
   },
   "outputs": [],
   "source": [
    "# Download DiffWave pre-trained model from - https://github.com/lmnt-com/diffwave/blob/master/README.md\n",
    "model_dir = r'diffwave-ljspeech-22kHz-1000578.pt' # Change to model dir\n",
    "print('Mel-Spectrogram to Audio')\n",
    "mel_spectrogram = torch.tensor(maskt_Sxx_full, dtype=torch.float)\n",
    "audio, sample_rate = diffwave_predict(mel_spectrogram, model_dir, device=device, fast_sampling=True) # device=torch.device('cuda') in case of using GPU\n",
    "\n",
    "print('Save Reconstructed Audio')\n",
    "reconstructed_path = output_audio_dir / \"proposed_method_reconst.wav\"\n",
    "T.save(reconstructed_path, audio.cpu(), sample_rate=sample_rate)\n",
    "\n",
    "if single_frame:\n",
    "    # read from clean_output_dir / \"np_arrays\" the first frame spectrogram into a vraiable and convert it to tensor float\n",
    "    clean_Sxx = np.load(clean_output_dir / \"np_arrays\" / \"0000.npy\")\n",
    "    clean_Sxx = torch.tensor(clean_Sxx, dtype=torch.float)\n",
    "    # reconstruct the audio from the clean spectrogram\n",
    "    reconstr_clean_audio, _ = diffwave_predict(clean_Sxx, model_dir, device=device, fast_sampling=True)\n",
    "    baseline_diff_wave_path = output_audio_dir / \"baseline_diff_wave_reconst.wav\"\n",
    "    T.save(baseline_diff_wave_path, reconstr_clean_audio.cpu(), sample_rate=sample_rate)\n",
    "\n",
    "\n",
    "\n",
    "# reconstruct with spectral gating and compute metrics\n",
    "if add_noise and single_frame:\n",
    "    spec_gate_recon_path = output_audio_dir / \"spectral_gating_recon.wav\"\n",
    "    utils.reduce_noise(noisy_audio_path, spec_gate_recon_path, decrease_factor=0.9)\n",
    "    utils.print_quality_metrics(audio_path, noisy_audio_path, reconstructed_path,\n",
    "                                 spec_gate_recon_path, baseline_diff_wave_path)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
