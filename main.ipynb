{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e7a0db5-7f04-4845-8b11-684fe6e9f7f2",
   "metadata": {
    "id": "6e7a0db5-7f04-4845-8b11-684fe6e9f7f2"
   },
   "source": [
    "# Video segmentation with SAM 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ba7875-35e5-478b-b8ba-4b48e121dec7",
   "metadata": {
    "id": "73ba7875-35e5-478b-b8ba-4b48e121dec7"
   },
   "source": [
    "This notebook shows how to use SAM 2 for interactive segmentation in videos. It will cover the following:\n",
    "\n",
    "- adding clicks (or box) on a frame to get and refine _masklets_ (spatio-temporal masks)\n",
    "- propagating clicks (or box) to get _masklets_ throughout the video\n",
    "- segmenting and tracking multiple objects at the same time\n",
    "\n",
    "We use the terms _segment_ or _mask_ to refer to the model prediction for an object on a single frame, and _masklet_ to refer to the spatio-temporal masks across the entire video."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a887b90f-6576-4ef8-964e-76d3a156ccb6",
   "metadata": {
    "id": "a887b90f-6576-4ef8-964e-76d3a156ccb6"
   },
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/facebookresearch/sam2/blob/main/notebooks/video_predictor_example.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26616201-06df-435b-98fd-ad17c373bb4a",
   "metadata": {
    "id": "26616201-06df-435b-98fd-ad17c373bb4a"
   },
   "source": [
    "## 1. Environment Set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8491a127-4c01-48f5-9dc5-f148a9417fdf",
   "metadata": {
    "id": "8491a127-4c01-48f5-9dc5-f148a9417fdf"
   },
   "source": [
    "If running locally using jupyter, first install `sam2` in your environment using the [installation instructions](https://github.com/facebookresearch/sam2#installation) in the repository.\n",
    "\n",
    "If running from Google Colab, set `using_colab=True` below and run the cell. In Colab, be sure to select 'GPU' under 'Edit'->'Notebook Settings'->'Hardware accelerator'. Note that it's recommended to use **A100 or L4 GPUs when running in Colab** (T4 GPUs might also work, but could be slow and might run out of memory in some cases)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74c53be-aab1-46b9-8c0b-068b52ef5948",
   "metadata": {
    "id": "f74c53be-aab1-46b9-8c0b-068b52ef5948"
   },
   "outputs": [],
   "source": [
    "# Parameters setting:\n",
    "using_colab = False\n",
    "\n",
    "\n",
    "single_frame = True\n",
    "add_noise = False # currently, when set to true, it will save the first frame as clean and make the other frames noisy. It does it by creating mixed dirs created in the code\n",
    "sigma = 0.05\n",
    "deafult_saving_dpi = 600\n",
    "deafult_printing_dpi = 600\n",
    "audio_path = r'audio_example.wav'  # Replace with your audio file path\n",
    "output_dir = 'spectrogram'  # Replace with your output directory path\n",
    "sam2_checkpoint = \"checkpoints/sam2.1_hiera_large.pt\"\n",
    "model_cfg = \"configs/sam2.1/sam2.1_hiera_l.yaml\"\n",
    "\n",
    "noisy_audio_path = \"noisy_\" + audio_path\n",
    "noisy_output_dir = \"noisy_\" + output_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d824a4b2-71f3-4da3-bfc7-3249625e6730",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d824a4b2-71f3-4da3-bfc7-3249625e6730",
    "outputId": "666ad841-a007-4410-e88f-6c6d92039341"
   },
   "outputs": [],
   "source": [
    "if using_colab:\n",
    "    import torch\n",
    "    import torchvision\n",
    "    print(\"PyTorch version:\", torch.__version__)\n",
    "    print(\"Torchvision version:\", torchvision.__version__)\n",
    "    print(\"CUDA is available:\", torch.cuda.is_available())\n",
    "    import sys\n",
    "    !{sys.executable} -m pip install opencv-python matplotlib\n",
    "    !{sys.executable} -m pip install 'git+https://github.com/facebookresearch/sam2.git'\n",
    "\n",
    "    !mkdir -p videos\n",
    "    !wget -P videos https://dl.fbaipublicfiles.com/segment_anything_2/assets/bedroom.zip\n",
    "    !unzip -d videos videos/bedroom.zip\n",
    "\n",
    "    !mkdir -p ../checkpoints/\n",
    "    !wget -P ../checkpoints/ https://dl.fbaipublicfiles.com/segment_anything_2/092824/sam2.1_hiera_large.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e6aa9d-487f-4207-b657-8cff0902343e",
   "metadata": {
    "id": "22e6aa9d-487f-4207-b657-8cff0902343e"
   },
   "source": [
    "## Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5318a85-5bf7-4880-b2b3-15e4db24d796",
   "metadata": {
    "id": "e5318a85-5bf7-4880-b2b3-15e4db24d796"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import shutil\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import torchaudio as T\n",
    "import torchaudio.transforms as TT\n",
    "from diffwave.inference import predict as diffwave_predict\n",
    "from diffwave.params import params\n",
    "import librosa.display\n",
    "from sam2.build_sam import build_sam2_video_predictor\n",
    "from pathlib import Path\n",
    "\n",
    "if sys.platform == \"darwin\": # This means if we are using macOS\n",
    "    os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "\n",
    "\n",
    "# setting global parameters for printing and saving the images. If we want to change it we need also to change it at SAM2 so leave it as is for the moment\n",
    "plt.rcParams['figure.figsize'] = [8, 8]\n",
    "plt.rcParams['figure.dpi'] = deafult_printing_dpi\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ba49d8-8c22-4eba-a2ab-46eee839287f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "08ba49d8-8c22-4eba-a2ab-46eee839287f",
    "outputId": "1598810b-2fd3-444b-ce43-6d7617e3bf25"
   },
   "outputs": [],
   "source": [
    "# select the device for computation\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "# elif torch.backends.mps.is_available():\n",
    "#     device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"using device: {device}\")\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    # use bfloat16 for the entire notebook\n",
    "    torch.autocast(\"cuda\", dtype=torch.bfloat16).__enter__()\n",
    "    # turn on tfloat32 for Ampere GPUs (https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices)\n",
    "    if torch.cuda.get_device_properties(0).major >= 8:\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "elif device.type == \"mps\":\n",
    "    print(\n",
    "        \"\\nSupport for MPS devices is preliminary. SAM 2 is trained with CUDA and might \"\n",
    "        \"give numerically different outputs and sometimes degraded performance on MPS. \"\n",
    "        \"See e.g. https://github.com/pytorch/pytorch/issues/84936 for a discussion.\"\n",
    "    )\n",
    "\n",
    "\n",
    "#delete spectrograms left from older runs\n",
    "if os.path.exists(output_dir + \"_images\"):\n",
    "    shutil.rmtree(output_dir + \"_images\")\n",
    "if os.path.exists(output_dir + \"_np_array\"):\n",
    "    shutil.rmtree(output_dir + \"_np_array\")\n",
    "if os.path.exists(noisy_output_dir + \"_images\"):\n",
    "    shutil.rmtree(noisy_output_dir + \"_images\")\n",
    "if os.path.exists(noisy_output_dir + \"_np_array\"):\n",
    "    shutil.rmtree(noisy_output_dir + \"_np_array\")\n",
    "if os.path.exists(f\"mixed_{output_dir}_images\"):\n",
    "    shutil.rmtree(f\"mixed_{output_dir}_images\")\n",
    "if os.path.exists(f\"mixed_{output_dir}_np_array\"):\n",
    "    shutil.rmtree(f\"mixed_{output_dir}_np_array\")\n",
    "\n",
    "\n",
    "# create a noisy version if needed\n",
    "if add_noise is True:\n",
    "    waveform, sample_rate = T.load(audio_path)\n",
    "    # Generate Gaussian noise\n",
    "    noise = torch.randn_like(waveform) * 0.005  # Std=0.05\n",
    "    # Add noise\n",
    "    noisy_waveform = waveform + noise\n",
    "    # Normalize to prevent clipping\n",
    "    noisy_waveform = torch.clamp(noisy_waveform, -1.0, 1.0)\n",
    "    # Save the noisy audio\n",
    "    T.save(noisy_audio_path, noisy_waveform, sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28adaa3f",
   "metadata": {
    "id": "28adaa3f"
   },
   "source": [
    "## 2. Creating a spectrogram from an audio file\n",
    "### There is no need to run this section if the files are already saved on the computer.\n",
    "We will define a function that converts an audio file to spectrogram files and spectrogram images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad787c42",
   "metadata": {
    "id": "ad787c42"
   },
   "outputs": [],
   "source": [
    "def input2mel(filename, output_dir):\n",
    "    # Load the original audio file\n",
    "    audio, sr = T.load(filename)  # Load audio and sample rate using torchaudio\n",
    "\n",
    "    # Check if resampling is needed\n",
    "    if sr != params.sample_rate:\n",
    "        resampler = TT.Resample(orig_freq=sr, new_freq=params.sample_rate)\n",
    "        audio = resampler(audio)\n",
    "        sr = params.sample_rate\n",
    "\n",
    "    # Check if reformating is needed\n",
    "    if audio.type() != torch.int16:\n",
    "        # Convert the audio to 16-bit PCM format\n",
    "        audio = (audio * 32767).to(torch.int16)\n",
    "\n",
    "    audio = torch.clamp(audio[0] / 32767.5, -1.0, 1.0)\n",
    "\n",
    "    window_size = params.hop_samples * 4     \n",
    "    overlap = params.hop_samples        \n",
    "    nfft = params.n_fft  \n",
    "\n",
    "    mel_args = {\n",
    "        'sample_rate': sr,\n",
    "        'win_length': window_size, \n",
    "        'hop_length': overlap,   \n",
    "        'n_fft': nfft,\n",
    "        'f_min': 20.0,\n",
    "        'f_max': sr / 2.0,\n",
    "        'n_mels': params.n_mels,\n",
    "        'power': 1.0,\n",
    "        'normalized': True,\n",
    "    }\n",
    "    mel_spec_transform = TT.MelSpectrogram(**mel_args)\n",
    "\n",
    "    duration = audio.size(0) / sr\n",
    "    # Calculate the total number of X-second chunks - for better interpretability\n",
    "    chunk_duration = duration  # Duration in seconds\n",
    "    chunk_samples = chunk_duration * sr\n",
    "    total_chunks = int(np.ceil(len(audio) / chunk_samples))\n",
    "\n",
    "    for chunk_idx in range(total_chunks):\n",
    "        # Extract the current segment\n",
    "         # Extract the current chunk\n",
    "        start_sample = int(chunk_idx * chunk_samples)\n",
    "        end_sample = int(min((chunk_idx + 1) * chunk_samples, len(audio)))\n",
    "        chunk = audio[start_sample:end_sample]\n",
    "\n",
    "        # Pad the last chunk if it's less than X seconds\n",
    "        if len(chunk) < chunk_samples:\n",
    "            chunk = torch.cat((chunk, torch.zeros(chunk_samples - len(chunk))))\n",
    "\n",
    "\n",
    "        # Compute the spectrogram\n",
    "        with torch.no_grad():\n",
    "            spectrogram = mel_spec_transform(chunk)\n",
    "            spectrogram = 20 * torch.log10(torch.clamp(spectrogram, min=1e-5)) - 20\n",
    "            spectrogram = torch.clamp((spectrogram + 100) / 100, 0.0, 1.0)\n",
    "\n",
    "\n",
    "        # Create figure with the correct aspect ratio\n",
    "        fig = plt.figure()\n",
    "        ax = plt.Axes(fig, [0., 0., 1., 1.])\n",
    "        fig.add_axes(ax)\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "        librosa.display.specshow(\n",
    "            spectrogram.numpy(),\n",
    "            sr=sr,\n",
    "            hop_length=overlap,\n",
    "            x_axis=\"time\",\n",
    "            y_axis=\"mel\",\n",
    "            cmap=\"magma\",  # Can try also 'jet'\n",
    "        )\n",
    "\n",
    "        output_dir_img = output_dir + \"_images\"\n",
    "        output_dir_spec = output_dir + \"_np_array\"\n",
    "        os.makedirs(output_dir_spec, exist_ok=True)\n",
    "        os.makedirs(output_dir_img, exist_ok=True)\n",
    "\n",
    "        # Save the spectrogram image\n",
    "        output_path_img = os.path.join(output_dir_img, f'{chunk_idx:04d}.jpg')\n",
    "        fig.savefig(output_path_img, bbox_inches='tight', pad_inches=0)\n",
    "        plt.close(fig)\n",
    "\n",
    "        # Save the original spectrogram data\n",
    "        output_path_spec = os.path.join(output_dir_spec, f'{chunk_idx:04d}.npy')\n",
    "        np.save(output_path_spec, spectrogram.numpy()) # The spectrogram is a numpy array\n",
    "    return sr, overlap, output_dir_img\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e077bc",
   "metadata": {
    "id": "08e077bc"
   },
   "source": [
    "Upload an audio file and create a spectrogram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8037794",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d8037794",
    "outputId": "e29e8077-6792-4409-8ad7-3bccf7a669fd"
   },
   "outputs": [],
   "source": [
    "sr, overlap, video_dir = input2mel(audio_path, output_dir)\n",
    "\n",
    "print(video_dir)\n",
    "# if add_noise is true, creates mixed dirs where the first frame is clean and the rest are noisy. then, assign video_dir to mixed_video_dir\n",
    "if add_noise is True:\n",
    "    sr, overlap, noisy_video_dir = input2mel(noisy_audio_path, noisy_output_dir)\n",
    "    \n",
    "    # Create Path objects for all directories\n",
    "    mixed_images_dir = Path(f\"mixed_{output_dir}_images\")\n",
    "    mixed_np_dir = Path(f\"mixed_{output_dir}_np_array\")\n",
    "    clean_images_dir = Path(output_dir + \"_images\")\n",
    "    clean_np_dir = Path(output_dir + \"_np_array\")\n",
    "    noisy_images_dir = Path(noisy_output_dir + \"_images\")\n",
    "    noisy_np_dir = Path(noisy_output_dir + \"_np_array\")\n",
    "    \n",
    "    # Create the directories if they don't exist\n",
    "    mixed_images_dir.mkdir(exist_ok=True)\n",
    "    mixed_np_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Copy files for images using PIL\n",
    "    # Copy 0000.jpg from clean directory\n",
    "    img = Image.open(clean_images_dir / \"0000.jpg\")\n",
    "    img.save(mixed_images_dir / \"0000.jpg\", \"JPEG\", quality=100)\n",
    "    \n",
    "    # Copy all non-0000 files from noisy directory\n",
    "    for image_file in noisy_images_dir.glob(\"*.jpg\"):\n",
    "        if image_file.name != \"0000.jpg\":\n",
    "            img = Image.open(image_file)\n",
    "            img.save(mixed_images_dir / image_file.name, \"JPEG\", quality=100)\n",
    "    \n",
    "    # Copy files for numpy arrays\n",
    "    # Copy 0000.npy from clean directory\n",
    "    shutil.copy2(\n",
    "        clean_np_dir / \"0000.npy\",\n",
    "        mixed_np_dir / \"0000.npy\"\n",
    "    )\n",
    "    \n",
    "    # Copy all non-0000 files from noisy directory\n",
    "    for np_file in noisy_np_dir.glob(\"*.npy\"):\n",
    "        if np_file.name != \"0000.npy\":\n",
    "            shutil.copy2(\n",
    "                np_file,\n",
    "                mixed_np_dir / np_file.name\n",
    "            )\n",
    "\n",
    "    if not single_frame:\n",
    "        video_dir = str(mixed_images_dir)\n",
    "    else:\n",
    "        video_dir = str(noisy_images_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8e0779-751f-4224-9b04-ed0f0b406500",
   "metadata": {
    "id": "ae8e0779-751f-4224-9b04-ed0f0b406500"
   },
   "source": [
    "## 3. Segmentation using SAM2\n",
    "### Loading the SAM 2 video predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f3245e-b4d6-418b-a42a-a67e0b3b5aec",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 339
    },
    "id": "f5f3245e-b4d6-418b-a42a-a67e0b3b5aec",
    "outputId": "52726ce5-cdcd-45f3-88bb-23097038dcdb"
   },
   "outputs": [],
   "source": [
    "predictor = build_sam2_video_predictor(model_cfg, sam2_checkpoint, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5320fe-06d7-45b8-b888-ae00799d07fa",
   "metadata": {
    "id": "1a5320fe-06d7-45b8-b888-ae00799d07fa"
   },
   "outputs": [],
   "source": [
    "def show_mask(mask, ax, obj_id=None, random_color=False):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        cmap = plt.get_cmap(\"tab10\")\n",
    "        cmap_idx = 0 if obj_id is None else obj_id\n",
    "        color = np.array([*cmap(cmap_idx)[:3], 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)\n",
    "\n",
    "\n",
    "def show_points(coords, labels, ax, marker_size=200):\n",
    "    pos_points = coords[labels==1]\n",
    "    neg_points = coords[labels==0]\n",
    "    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
    "    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
    "\n",
    "\n",
    "def show_box(box, ax):\n",
    "    x0, y0 = box[0], box[1]\n",
    "    w, h = box[2] - box[0], box[3] - box[1]\n",
    "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0, 0, 0, 0), lw=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22aa751-b7cd-451e-9ded-fb98bf4bdfad",
   "metadata": {
    "id": "f22aa751-b7cd-451e-9ded-fb98bf4bdfad"
   },
   "source": [
    "#### Upload the spectrogram images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4c6af6-e18d-4939-beaf-2bc00f94a724",
   "metadata": {
    "id": "1c4c6af6-e18d-4939-beaf-2bc00f94a724"
   },
   "source": [
    "We will upload one picture to make sure everything is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94c87ca-fd1a-4011-9609-e8be1cbe3230",
   "metadata": {
    "id": "b94c87ca-fd1a-4011-9609-e8be1cbe3230",
    "outputId": "e4ab13d1-f297-4d82-8513-3c1f220c332d"
   },
   "outputs": [],
   "source": [
    "# scan all the JPEG frame names in the frames directory\n",
    "frame_names = [\n",
    "    p for p in os.listdir(video_dir)\n",
    "    if os.path.splitext(p)[-1] in [\".jpg\", \".jpeg\", \".JPG\", \".JPEG\"]\n",
    "]\n",
    "frame_names.sort(key=lambda p: int(os.path.splitext(p)[0]))\n",
    "\n",
    "# take a look the first video frame\n",
    "frame_idx = 0\n",
    "plt.title(f\"frame {frame_idx}\")\n",
    "plt.imshow(Image.open(os.path.join(video_dir, frame_names[frame_idx])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff46b10-c17a-4a26-8004-8c6d80806b0a",
   "metadata": {
    "id": "dff46b10-c17a-4a26-8004-8c6d80806b0a"
   },
   "source": [
    "#### Initialize the inference state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f594ac71-a6b9-461d-af27-500fa1d1a420",
   "metadata": {
    "id": "f594ac71-a6b9-461d-af27-500fa1d1a420"
   },
   "source": [
    "SAM 2 requires stateful inference for interactive video segmentation, so we need to initialize an **inference state** on this video.\n",
    "\n",
    "During initialization, it loads all the JPEG frames in `video_path` and stores their pixels in `inference_state` (as shown in the progress bar below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8967aed3-eb82-4866-b8df-0f4743255c2c",
   "metadata": {
    "id": "8967aed3-eb82-4866-b8df-0f4743255c2c",
    "outputId": "a59a0e3c-5cc4-44d9-c9b0-f91ac6c5e1c1"
   },
   "outputs": [],
   "source": [
    "inference_state = predictor.init_state(video_path=video_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2d3127-67b2-45d2-9f32-8fe3e10dc5eb",
   "metadata": {
    "id": "aa2d3127-67b2-45d2-9f32-8fe3e10dc5eb"
   },
   "source": [
    "Note: if you have run any previous tracking using this `inference_state`, please reset it first via `reset_state`.\n",
    "\n",
    "(The cell below is just for illustration; it's not needed to call `reset_state` here as this `inference_state` is just freshly initialized above.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2646a1d-3401-438c-a653-55e0e56b7d9d",
   "metadata": {
    "id": "d2646a1d-3401-438c-a653-55e0e56b7d9d"
   },
   "outputs": [],
   "source": [
    "predictor.reset_state(inference_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da018be8-a4ae-4943-b1ff-702c2b89cb68",
   "metadata": {
    "id": "da018be8-a4ae-4943-b1ff-702c2b89cb68"
   },
   "source": [
    "### Segment multiple objects simultaneously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c542cfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from clicker import collect_clicks\n",
    "\n",
    "prompts = {}  # hold all the clicks we add for visualization\n",
    "ann_frame_idx = 0  # the frame index we interact with\\\n",
    "\n",
    "img_path = os.path.join(video_dir, frame_names[ann_frame_idx])\n",
    "\n",
    "points_list, labels = collect_clicks(img_path)\n",
    "\n",
    "assert len(points_list) == len(labels), \"Number of points and labels should be the same\"\n",
    "\n",
    "num_of_promt = len(points_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19166c4d",
   "metadata": {
    "id": "19166c4d"
   },
   "source": [
    "#### Step 1: Add clicks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ecf61d-662b-4f98-ae62-46557b219842",
   "metadata": {
    "id": "95ecf61d-662b-4f98-ae62-46557b219842",
    "outputId": "481be066-a984-4d09-8ed9-3137e52abd3a"
   },
   "outputs": [],
   "source": [
    "# # Example of how the points and labels should be set up manually\n",
    "\n",
    "# # sending all clicks (and their labels) to `add_new_points_or_box`\n",
    "# #points_list = [np.array([[600, 170], [275, 800]], dtype=np.float32)] # Exempale\n",
    "# points_list = [np.array([[800, 2100]], dtype=np.float32), np.array([[1500, 2120]], dtype=np.float32)] #TODO: add clicks\n",
    "# num_of_promt = len(points_list)\n",
    "\n",
    "# # for labels, `1` means positive click and `0` means negative click\n",
    "# #labels = [np.array([1, 0], np.int32),np.array([1], np.int32)] # Exempale\n",
    "# labels = [np.array([1], np.int32),np.array([1], np.int32)] #TODO: add labels\n",
    "\n",
    "for i in range(num_of_promt):\n",
    "    prompts[i] = points_list[i], labels[i]\n",
    "    _, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(\n",
    "        inference_state=inference_state,\n",
    "        frame_idx=0,\n",
    "        obj_id=i,\n",
    "        points=points_list[i],\n",
    "        labels=labels[i],\n",
    ")\n",
    "\n",
    "\n",
    "img = Image.open(os.path.join(video_dir, frame_names[ann_frame_idx]))\n",
    "w, h = img.size\n",
    "aspect_ratio = w / h\n",
    "\n",
    "# Create figure with the correct aspect ratio\n",
    "fig = plt.figure()\n",
    "ax = plt.Axes(fig, [0., 0., 1., 1.])\n",
    "fig.add_axes(ax)\n",
    "\n",
    "# Add title (optionally adjust its position if needed)\n",
    "ax.set_title(f\"frame {ann_frame_idx}\", pad=20)\n",
    "\n",
    "# Show image and rest of visualization\n",
    "ax.imshow(img)\n",
    "for i in range(num_of_promt):\n",
    "    show_points(points_list[i], labels[i], ax)\n",
    "    for i, out_obj_id in enumerate(out_obj_ids):\n",
    "        show_points(*prompts[i], ax)\n",
    "        show_mask((out_mask_logits[i] > 0.0).cpu().numpy(), ax, obj_id=out_obj_id)\n",
    "\n",
    "ax.axis('off')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448733b8-ea8b-4078-995f-b676c3b558ba",
   "metadata": {
    "id": "448733b8-ea8b-4078-995f-b676c3b558ba"
   },
   "source": [
    "#### Step 2: Propagate the prompts to get masklets across the video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bd73de-d669-41c8-b6ba-943883f0caa2",
   "metadata": {
    "id": "60bd73de-d669-41c8-b6ba-943883f0caa2"
   },
   "source": [
    "Now, we propagate the prompts for both objects to get their masklets throughout the video.\n",
    "\n",
    "Note: when there are multiple objects, the `propagate_in_video` API will return a list of masks for each object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17737191-d62b-4611-b2c6-6d0418a9ab74",
   "metadata": {
    "id": "17737191-d62b-4611-b2c6-6d0418a9ab74",
    "outputId": "39da9e09-da2f-4de9-810b-882407541f54"
   },
   "outputs": [],
   "source": [
    "if not single_frame:\n",
    "    # run propagation throughout the video and collect the results in a dict\n",
    "    video_segments = {}  # video_segments contains the per-frame segmentation results\n",
    "    for out_frame_idx, out_obj_ids, out_mask_logits in predictor.propagate_in_video(inference_state):\n",
    "        video_segments[out_frame_idx] = {\n",
    "            out_obj_id: (out_mask_logits[i] > 0.0).cpu().numpy()\n",
    "            for i, out_obj_id in enumerate(out_obj_ids)\n",
    "        }\n",
    "\n",
    "    # render the segmentation results every few frames\n",
    "    vis_frame_stride = 1 # Should be reduced if the number of frames is large\n",
    "\n",
    "    plt.ioff()  # Turn off interactive mode for printing in the for loop\n",
    "\n",
    "    for out_frame_idx in range(0, len(frame_names), vis_frame_stride):\n",
    "        # Create a new figure for each frame\n",
    "        fig = plt.figure(figsize=(8, 8))\n",
    "        plt.title(f\"frame {out_frame_idx}\")\n",
    "        plt.imshow(Image.open(os.path.join(video_dir, frame_names[out_frame_idx])))\n",
    "        \n",
    "        for out_obj_id, out_mask in video_segments[out_frame_idx].items():\n",
    "            show_mask(out_mask, plt.gca(), obj_id=out_obj_id)\n",
    "        \n",
    "        plt.show()  # Display the current figure\n",
    "        plt.close(fig)  # Close the figure to free memory\n",
    "\n",
    "    plt.ion()  # Turn interactive mode back on"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9454d0",
   "metadata": {
    "id": "ec9454d0"
   },
   "source": [
    "## 4. Applying the masks to the original spectrograms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973fa1d2",
   "metadata": {
    "id": "973fa1d2"
   },
   "source": [
    "Now we will fit the mask that sam made to the original spectrogram.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27570e78",
   "metadata": {
    "id": "27570e78"
   },
   "source": [
    "First we will define a function that makes a linear transformation from a position in the image to a position in the spectrogram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6931850",
   "metadata": {
    "id": "c6931850"
   },
   "outputs": [],
   "source": [
    "def pixel_to_spectrogram(pixel_x, pixel_y, img_height, img_width, time_bins, freq_bins):\n",
    "    # Flip y-axis (since images are top-left origin)\n",
    "    spectrogram_freq_bin = freq_bins - int(pixel_y * freq_bins / img_height) - 1\n",
    "    spectrogram_time_bin = int(pixel_x * time_bins / img_width)\n",
    "\n",
    "    # Ensure indices are within bounds\n",
    "    spectrogram_freq_bin = np.clip(spectrogram_freq_bin, 0, freq_bins - 1)\n",
    "    spectrogram_time_bin = np.clip(spectrogram_time_bin, 0, time_bins - 1)\n",
    "\n",
    "    return spectrogram_time_bin, spectrogram_freq_bin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e49342a",
   "metadata": {
    "id": "6e49342a"
   },
   "source": [
    "### We will upload the original spectrograms and apply the mask on them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2618a51",
   "metadata": {
    "id": "e2618a51",
    "outputId": "74a7d140-c617-4270-ca0b-0ffe656e2568"
   },
   "outputs": [],
   "source": [
    "# #uplode the original spectogram\n",
    "# output_dir_spec = 'spectrogram_np_array/'\n",
    "\n",
    "# spectogram_names = [\n",
    "#     p for p in os.listdir(output_dir_spec)\n",
    "#     if os.path.splitext(p)[-1] in [\".npy\"]\n",
    "# ]\n",
    "# spectogram_names.sort(key=lambda p: int(os.path.splitext(p)[0]))\n",
    "\n",
    "# maskt_Sxx_full = None\n",
    "# for i in range(0, len(spectogram_names)):\n",
    "#     with open(output_dir_spec + spectogram_names[i], 'rb') as f:\n",
    "#         Sxx = np.load(f)\n",
    "\n",
    "#     # Compute the mapping\n",
    "#     freq_bins, time_bins = Sxx.shape\n",
    "\n",
    "#     inx_list = []\n",
    "#     for j in range(num_of_promt):\n",
    "#         mask = video_segments[i][j][0]\n",
    "#         # Mask dimensions\n",
    "#         mask_height, mask_width = mask.shape\n",
    "#         mask_inx = np.where(mask)\n",
    "#         mask_inx_list = list(zip(mask_inx[1], mask_inx[0]))\n",
    "\n",
    "#         for (x,y) in mask_inx_list:\n",
    "#             a,b = pixel_to_spectrogram(x, y, mask_height, mask_width, time_bins, freq_bins)\n",
    "#             if (a, b) not in inx_list:  # Check if (x, y) is already in the list\n",
    "#                         inx_list.append((a, b))\n",
    "\n",
    "#     mask_to_spec = np.zeros_like(Sxx, dtype=bool)\n",
    "#     for (x,y) in inx_list:\n",
    "#         mask_to_spec[y,x] = True\n",
    "\n",
    "#     maskt_Sxx = Sxx\n",
    "#     maskt_Sxx[~mask_to_spec] = Sxx.min()\n",
    "#     # Plot the spectrogram\n",
    "#     freq_bins, time_bins = maskt_Sxx.shape\n",
    "#     t = np.arange(0, time_bins, 1)\n",
    "#     f = np.arange(0, freq_bins, 1)\n",
    "\n",
    "#     if maskt_Sxx_full is None:\n",
    "#         maskt_Sxx_full = maskt_Sxx\n",
    "#     else:\n",
    "#         maskt_Sxx_full = np.concatenate((maskt_Sxx_full, maskt_Sxx), axis=1)\n",
    "\n",
    "\n",
    "#     # Plot the spectrogram using librosa\n",
    "#     fig = plt.figure()\n",
    "#     ax = plt.Axes(fig, [0., 0., 1., 1.])  # Remove margins for full screen\n",
    "#     fig.add_axes(ax)\n",
    "#     ax.axis(\"off\")\n",
    "\n",
    "#     librosa.display.specshow(\n",
    "#         maskt_Sxx,\n",
    "#         sr=sr,\n",
    "#         hop_length=overlap,\n",
    "#         x_axis=\"time\",\n",
    "#         y_axis=\"mel\",\n",
    "#         cmap=\"magma\",  # Can try also 'jet'\n",
    "#     )\n",
    "\n",
    "# print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcb5bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_to_spectrogram(pixel_x, pixel_y, img_height, img_width, time_bins, freq_bins):\n",
    "    \"\"\"\n",
    "    Map matrices of pixel coordinates (pixel_x, pixel_y) to spectrogram indices.\n",
    "\n",
    "    Args:\n",
    "        pixel_x (np.ndarray): Array of x-coordinates in the image.\n",
    "        pixel_y (np.ndarray): Array of y-coordinates in the image.\n",
    "        img_height (int): Height of the image.\n",
    "        img_width (int): Width of the image.\n",
    "        time_bins (int): Number of time bins in the spectrogram.\n",
    "        freq_bins (int): Number of frequency bins in the spectrogram.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Array of time indices for the spectrogram.\n",
    "        np.ndarray: Array of frequency indices for the spectrogram.\n",
    "    \"\"\"\n",
    "    # Flip y-axis (since images are top-left origin)\n",
    "    spectrogram_freq_bin = freq_bins - ((pixel_y * freq_bins / img_height).astype(int)) - 1\n",
    "    spectrogram_time_bin = (pixel_x * time_bins / img_width).astype(int)\n",
    "\n",
    "    # Ensure indices are within bounds\n",
    "    spectrogram_freq_bin = np.clip(spectrogram_freq_bin, 0, freq_bins - 1)\n",
    "    spectrogram_time_bin = np.clip(spectrogram_time_bin, 0, time_bins - 1)\n",
    "\n",
    "    return spectrogram_time_bin, spectrogram_freq_bin\n",
    "\n",
    "\n",
    "### apply the masks to each frame\n",
    "\n",
    "\n",
    "# Directory containing spectrogram numpy arrays\n",
    "if add_noise is True:\n",
    "    output_dir_spec = f\"mixed_{output_dir}_np_array\"\n",
    "else:\n",
    "    output_dir_spec = output_dir + \"_np_array\"\n",
    "\n",
    "# Get and sort spectrogram file names\n",
    "spectrogram_files = [\n",
    "    p for p in os.listdir(output_dir_spec) if p.endswith(\".npy\")\n",
    "]\n",
    "spectrogram_files.sort(key=lambda p: int(os.path.splitext(p)[0]))\n",
    "\n",
    "# Initialize the full masked spectrogram\n",
    "maskt_Sxx_full = []\n",
    "\n",
    "for i, file_name in enumerate(spectrogram_files):\n",
    "    file_path = os.path.join(output_dir_spec, file_name)\n",
    "\n",
    "    # Load spectrogram data\n",
    "    Sxx = np.load(file_path)\n",
    "    freq_bins, time_bins = Sxx.shape\n",
    "\n",
    "    # Matrix-wise processing for all masks\n",
    "    inx_list = []\n",
    "    for j in range(num_of_promt):\n",
    "        mask = video_segments[i][j][0]\n",
    "        mask_height, mask_width = mask.shape\n",
    "\n",
    "        # Extract indices where the mask is True\n",
    "        mask_inx = np.where(mask)  # Returns arrays of y and x indices\n",
    "        mask_x, mask_y = mask_inx[1], mask_inx[0]  # Extract x and y coordinates\n",
    "\n",
    "        # Use matrix-wise function to map pixel coordinates to spectrogram indices\n",
    "        spec_time_bins, spec_freq_bins = matrix_to_spectrogram(\n",
    "            mask_x, mask_y, mask_height, mask_width, time_bins, freq_bins\n",
    "        )\n",
    "\n",
    "        # Add unique indices to the global list\n",
    "        unique_indices = np.unique(np.stack((spec_time_bins, spec_freq_bins), axis=1), axis=0)\n",
    "        inx_list.append(unique_indices)\n",
    "\n",
    "    # Combine all unique indices for this spectrogram\n",
    "    inx_array = np.vstack(inx_list)\n",
    "\n",
    "    # Create the spectrogram mask\n",
    "    mask_to_spec = np.zeros_like(Sxx, dtype=bool)\n",
    "    mask_to_spec[inx_array[:, 1], inx_array[:, 0]] = True\n",
    "\n",
    "    # Apply mask to the spectrogram\n",
    "    maskt_Sxx = Sxx.copy()\n",
    "    maskt_Sxx[~mask_to_spec] = Sxx.min()\n",
    "\n",
    "    # Append to full spectrogram list\n",
    "    maskt_Sxx_full.append(maskt_Sxx)\n",
    "\n",
    "    # Plot the masked spectrogram\n",
    "    plt.figure(figsize=(4, 4), dpi=300)\n",
    "    librosa.display.specshow(\n",
    "        maskt_Sxx,\n",
    "        sr=sr,\n",
    "        hop_length=overlap,\n",
    "        x_axis=\"time\",\n",
    "        y_axis=\"mel\",\n",
    "        cmap=\"magma\",\n",
    "    )\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "# Concatenate all spectrograms along the time axis\n",
    "maskt_Sxx_full = np.concatenate(maskt_Sxx_full, axis=1)\n",
    "print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe68a9ad",
   "metadata": {
    "id": "fe68a9ad"
   },
   "source": [
    "Let's see the final maskt spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaab43eb",
   "metadata": {
    "id": "aaab43eb",
    "outputId": "61c54664-67d1-4650-8e2c-fd7f4e199b0d"
   },
   "outputs": [],
   "source": [
    "# Plot the spectrogram using librosa\n",
    "fig = plt.figure()  # 1024x1024 pixels at 300 DPI\n",
    "ax = plt.Axes(fig, [0., 0., 1., 1.])  # Remove margins for full screen\n",
    "fig.add_axes(ax)\n",
    "ax.axis(\"off\")\n",
    "\n",
    "librosa.display.specshow(\n",
    "    maskt_Sxx_full,\n",
    "    sr=sr,\n",
    "    hop_length=overlap,\n",
    "    x_axis=\"time\",\n",
    "    y_axis=\"mel\",\n",
    "    cmap=\"magma\",  # Can try also 'jet'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1087dbd",
   "metadata": {
    "id": "d1087dbd"
   },
   "source": [
    "# 5. Reconstruction the audio from the maskt spectrograms using DiffWave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HgjXWy54NL3G",
   "metadata": {
    "id": "HgjXWy54NL3G"
   },
   "outputs": [],
   "source": [
    "# Download DiffWave pre-trained model from - https://github.com/lmnt-com/diffwave/blob/master/README.md\n",
    "model_dir = r'diffwave-ljspeech-22kHz-1000578.pt' # Change to your directory\n",
    "print('Mel-Spectrogram to Audio')\n",
    "mel_spectrogram = torch.tensor(maskt_Sxx_full)\n",
    "audio, sample_rate = diffwave_predict(mel_spectrogram, model_dir, device=device, fast_sampling=True) # device=torch.device('cuda') in case of using GPU\n",
    "\n",
    "print('Save Reconstructed Audio')\n",
    "T.save(\"reconstructed_audio.wav\", audio.cpu(), sample_rate=sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mF8gzdgLQ3Sl",
   "metadata": {
    "id": "mF8gzdgLQ3Sl"
   },
   "source": [
    "## 6. Play the Reconstructed Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5maZreL0RRxI",
   "metadata": {
    "id": "5maZreL0RRxI"
   },
   "outputs": [],
   "source": [
    "# import sounddevice as sd\n",
    "# # Play the audio_resampled\n",
    "# sd.play(audio.squeeze().numpy(), samplerate=sample_rate)\n",
    "\n",
    "# # Wait until playback is finished\n",
    "# sd.wait()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
